"# ai_embedded_assistant" 

Prerequisition:

- Python 3.9 or later installed.
- Installed dependencies: pip install pdfplumber nltk sentence-transformers chromadb streamlit spacy requests
- Downloaded NLTK data: import nltk; nltk.download('punkt')
- Installed spaCy’s small English model: python -m spacy download en_core_web_sm
- Installed and started Ollama with Mistral-7B loaded: ollama serve & and ollama run mistral
- Created a project folder with a documents/ subfolder containing your PDF files.

=================================================================

Steps to Set Up

1. Create the documents/ folder in your project directory and add your PDF files.

2. Run preprocess.py to process the documents and generate embeddings (stored in embeddings/).

3. Ensure Ollama is running with Mistral-7B loaded. You can start it with:
ollama serve &
ollama run mistral

4. Run app.py to launch the Streamlit UI and start using the chatbot:
streamlit run app.py

=================================================================
Description

documents/: Place all your PDF files here (e.g., whitepaper1.pdf, ebook2.pdf). The preprocess.py script will read and process these files.
embeddings/: This folder is automatically created when you run preprocess.py. It stores the Chroma vector database with the embeddings of your document chunks, so you don’t need to create it manually.
preprocess.py: This script processes the PDFs, extracts text, chunks it, applies semantic enrichment, embeds the chunks, and stores them in Chroma.
query.py: This handles user queries, retrieves relevant document chunks from Chroma, and generates answers using the Mistral-7B model via Ollama.
app.py: This sets up the Streamlit web interface, letting you interact with the chatbot through a browser.